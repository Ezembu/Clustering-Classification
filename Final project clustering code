!apt-get install openjdk-8-jdk-headless


!wget https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop2.7.tgz

!tar xf /content/spark-3.2.1-bin-hadoop2.7.tgz

!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.2.1-bin-hadoop2.7"

import findspark
findspark.init()
findspark.find()

import pyspark
import numpy as np
import pandas as pd

combined_data_df = pd.read_csv('/content/combined-data.csv')
combined_data_df['label'] = combined_data_df['avg_price'].apply(lambda x: 0 if x < 5 else 1)

counts = combined_data_df['label'].value_counts()
print(counts)


counts = combined_data_df.groupby('platformType')['label'].value_counts()
print(counts)

combined_data_df = combined_data_df.dropna()

combined_data_df.head()

from pyspark.sql import SparkSession
from pyspark.ml.clustering import KMeans
from pyspark.ml.feature import VectorAssembler
from pyspark.sql.functions import col
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml import Pipeline

# Create a SparkSession
spark = SparkSession.builder.appName("Clustering").getOrCreate()

# Read the CSV file into a DataFrame
df = spark.read.csv("/content/combined-data.csv", header=True, inferSchema=True)

df = df.withColumn("count_buyid", col("count_buyid").cast("double"))
df = df.withColumn("avg_price", col("avg_price").cast("double"))#
df = df.dropna()

# Select the relevant columns for clustering
selected_columns = ["count_gameclicks", "count_hits", "count_buyid", "avg_price"]
df_selected = df.select(selected_columns)



df_selected.filter(col("count_buyId").isNull()).count()


# Assemble the selected columns into a feature vector
assembler = VectorAssembler(inputCols=selected_columns, outputCol="features")
df_assembled = assembler.transform(df_selected)


df_assembled

from pyspark.sql.functions import isnan, count, when

df.select([count(when(isnan(c), c)).alias(c) for c in df.columns]).show()




from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.clustering import KMeans
from pyspark.ml import Pipeline

# set up each processing step with the correct input columns and output
assemble=VectorAssembler(inputCols=selected_columns, outputCol='features')
scale=StandardScaler(inputCol='features',outputCol='standardized')
km = KMeans(featuresCol = 'standardized')
pipe = Pipeline(stages=[assemble, scale, km])

# assemble the pipeline 
pipe = Pipeline(stages=[assemble, scale, km])

from pyspark.ml.evaluation import ClusteringEvaluator
from pyspark.sql.functions import col

df = df.withColumn("count_buyid", col("count_buyid").cast("double"))
df = df.withColumn("avg_price", col("avg_price").cast("double"))



# set up evaluator 
evaluator = ClusteringEvaluator()

# test between k=2 and 10 
for k in range(2,10):
  # set the KMeans stage of the pipe to hold each value of K and the random seed = 1 and fit that pipe to data  
  kmeans = pipe.getStages()[-1].setK(k).setSeed(1)  
  model = pipe.fit(df)
  
  # build a preds dataset of each k value
  preds = model.transform(df)

  # silhouette score each prediction set and print formatted output 
  silhouette = evaluator.evaluate(preds)
  print(f'Tested: {k} clusters: {silhouette}')

pipe.getStages()[-1].setK(3).setSeed(1)  # set the random seed for the algorithm and the value for k

# fit model and transform the data
model = pipe.fit(df)
clusters = model.transform(df)
clusters.show()

import plotly.express as px

vis_df = clusters.toPandas()

# build figure with 3D numeric dimensions and categorical isPlanet and prediction dimensions
fig = px.scatter_3d(vis_df, x='teamLevel', y='platformType', z='count_hits', color='prediction', symbol='count_buyid', template='ggplot2', hover_name='userId')
fig.show()
